{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing the Iris data and few modules\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "## loading the dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# making training data \n",
    "X_train = iris['data']\n",
    "# making testing data\n",
    "Y_train = iris['target']\n",
    "\n",
    "## converting into dataframe\n",
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "Y_train = pd.DataFrame(Y_train)\n",
    "\n",
    "# assigning columns names for easier referene\n",
    "X_train.columns = iris['feature_names']\n",
    "Y_train.columns = ['class']\n",
    "\n",
    "# creatinf features list for easier removal of feature after splitting on it\n",
    "features = list(X_train.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate entropy for us , the formula is implemented here \n",
    "\n",
    "def entropy(Y):\n",
    "    # number of samples belonging to class 0 , 1 , 2 are found respectively for calculating probability\n",
    "    a = len(Y[Y['class']==0])\n",
    "    b = len(Y[Y['class']==1])\n",
    "    c = len(Y[Y['class'] == 2])\n",
    "    entropy = 0 \n",
    "    # list made of these same values for  iterative ease \n",
    "    lis = [len(Y[Y['class']==0]),len(Y[Y['class']==1]),len(Y[Y['class'] == 2])]\n",
    "    for i in lis:\n",
    "        if i == 0:\n",
    "            entropy+=0\n",
    "            continue\n",
    "            #Entropy formula is implemented here for all the three clases , if a class dosent exisit zero is added \n",
    "        entropy += -((i/(a+b+c))*(math.log((i/(a+b+c)),10)))\n",
    "    return entropy\n",
    "\n",
    "# the below functions helps to find the gain it takes training data , target , feature to split on and the current entropy\n",
    "\n",
    "# simple formula calculation is implemented for each unique value of the X(training data) for a particular feature and we \n",
    "# will obviously select the feature and the value for which we get maximum gain in information , the same has been implemented\n",
    "   \n",
    "def gain(X,Y,feature,curr_entropy):\n",
    "    # unique values in a feature column for training data is realised\n",
    "    X_vals = set(X[feature])\n",
    "    en_val = 1000000\n",
    "    feat = ''\n",
    "    valu = 0\n",
    "    entro = 0\n",
    "    # finding minimum entropy for a value so we can get maximum gain \n",
    "    for val in X_vals:\n",
    "        entro = 0\n",
    "        index1 = pd.Index(X[feature]<val)\n",
    "        index2 = pd.Index(X[feature]>=val)\n",
    "        Y_for_that_val = Y[index1]\n",
    "        Y_not_for_that_val = Y[index2]\n",
    "        \n",
    "        # (len(Y[index?])/len(Y)) ==> this term is nothing but the weighted avergae for each entropy term\n",
    "        \n",
    "        entro = (len(Y[index1])/len(Y)) * entropy(Y_for_that_val) + (len(Y[index2])/len(Y)) * entropy(Y_not_for_that_val)\n",
    "        \n",
    "        # below if statements helps pick the minimum value of entropy \n",
    "        \n",
    "        if entro<en_val:\n",
    "            en_val = entro\n",
    "            valu = val\n",
    "    # gain is the difference of parent entropy - weighted average * child's entropy \n",
    "    # gain is calculated and returned along with value in feature for which highest gain is possible in split\n",
    "    gain = curr_entropy - en_val\n",
    "    return gain , valu\n",
    "    \n",
    "## this is the function to help call the recursion for children nodes\n",
    "\n",
    "def dt(X,Y,features,level):\n",
    "    \n",
    "    # if our targets are just of one type then we have encountered a pure node \n",
    "    if len(set(Y.iloc[:,0]))==1:\n",
    "        print('Level ' , level)\n",
    "        ans = -3\n",
    "        for x in set(Y.iloc[:,0]):\n",
    "            ans = x\n",
    "        print('Count of ',ans,\" is = \",len(Y))\n",
    "        print('Current Entropy is = 0.0')\n",
    "        print('Reached leaf Node')\n",
    "        print()\n",
    "        return \n",
    "        \n",
    "    # if there is no target then we should return because there is no data left \n",
    "    \n",
    "    if(len(set(Y.iloc[:,0]))==0):\n",
    "        return\n",
    "    \n",
    "    # this is for when we have run out of features to split on , here a majority vote is taken on the basis of target values\n",
    "    if(len(features)==0):\n",
    "        print('Level ' , level)\n",
    "        #finding the maximum number of a particular type of class present in this node\n",
    "        max_vals = max(Y['class'].value_counts())\n",
    "        count_val = -2\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                # finding out the class having maximum presence in the node \n",
    "                if Y['class'].value_counts()[i] == max_vals:\n",
    "                    count_val = i\n",
    "            except:\n",
    "                continue      \n",
    "        print('Count of ',count_val,\" is = \",max_vals)\n",
    "        print('Majority vote is taken here')\n",
    "        print('Current Entropy is = ', entropy(Y))\n",
    "        print('Reached leaf Node')\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    # If none of the above conditions occured then we are safe to proceed with the binary split because we have features\n",
    "    #  , data and a varied range of class values in our target so more splitting needs to be done\n",
    "    \n",
    "    # basic prints required for output \n",
    "    \n",
    "    print('Level ',level)\n",
    "    try:\n",
    "        count_0 = Y.iloc[:,0].value_counts().loc[0]\n",
    "    except:\n",
    "        count_0 = 0\n",
    "    print('Count of 0 = ',count_0)\n",
    "    try:\n",
    "        count_1 = Y.iloc[:,0].value_counts().loc[1]\n",
    "    except:\n",
    "        count_1 = 0\n",
    "    print('Count of 1 = ',count_1)\n",
    "    try:\n",
    "        count_2 = Y.iloc[:,0].value_counts().loc[2]\n",
    "    except:\n",
    "        count_2 = 0\n",
    "    print('Count of 2 = ',count_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # here we proceed to find the best feature to split on and the best value to split on in that feature\n",
    "    # we will simply pass all features and let the gain function do its job\n",
    "    # the feature with max gain will be selected\n",
    "    \n",
    "    max_gain = -100000\n",
    "    feature_to_split_on = ''\n",
    "    value_to_split_on = 0\n",
    "    \n",
    "    # initial entropy was found here\n",
    "    \n",
    "    current_entropy = entropy(Y)\n",
    "    \n",
    "    #calling gain on all features to find best feature to split on and the value\n",
    "    for f in features:\n",
    "        gain_of_splitting_on_f , value_to_split_on_1= gain(X,Y,f,current_entropy)\n",
    "        if(gain_of_splitting_on_f>max_gain):\n",
    "            max_gain = gain_of_splitting_on_f                     ## max gain stored here\n",
    "            feature_to_split_on = f                               ## feature name stored here\n",
    "            value_to_split_on = value_to_split_on_1               ## value to split on stored here\n",
    "    #Divide data for each of the two nodes one which is lesser than the value to split on for that feature\n",
    "    #One which is greater than or equal to the value to split on for that feature\n",
    "    \n",
    "    index1 = pd.Index(X[feature_to_split_on]<value_to_split_on)\n",
    "    index2 = pd.Index(X[feature_to_split_on]>=value_to_split_on)\n",
    "                      \n",
    "    # basic few prints for output \n",
    "    \n",
    "    print('Current entropy is ',current_entropy)\n",
    "    print('Splitting on feature ',feature_to_split_on,' with gain ratio ',max_gain)\n",
    "    print('Value to split on ',value_to_split_on)\n",
    "    print()\n",
    "   \n",
    "    # Since we already divided data for the feature to split on lets remove it from the features list so its not used again\n",
    "    features.remove(feature_to_split_on)\n",
    "    \n",
    "    ## Dividing training data and targets according to value mentioned above for the binary split\n",
    "                      \n",
    "    X_train_1 = X[X[feature_to_split_on]<value_to_split_on]\n",
    "    Y_train_1 = Y[index1]\n",
    "                      \n",
    "    # Calling recursion for the first child and also incrementing its level because it will be a child\n",
    "    dt(X_train_1,Y_train_1,features,level+1)\n",
    "                      \n",
    "    # Dividing data for the second child\n",
    "                      \n",
    "    X_train_2 = X[X[feature_to_split_on]>=value_to_split_on]\n",
    "    Y_train_2 = Y[index2]\n",
    "    # calling recursion for the second child \n",
    "    dt(X_train_2 , Y_train_2, features,level+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level  0\n",
      "Count of 0 =  50\n",
      "Count of 1 =  50\n",
      "Count of 2 =  50\n",
      "Current entropy is  0.4771212547196624\n",
      "Splitting on feature  petal length (cm)  with gain ratio  0.27643459094367495\n",
      "Value to split on  3.0\n",
      "\n",
      "Level  1\n",
      "Count of  0  is =  50\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level  1\n",
      "Count of 0 =  0\n",
      "Count of 1 =  50\n",
      "Count of 2 =  50\n",
      "Current entropy is  0.30102999566398114\n",
      "Splitting on feature  petal width (cm)  with gain ratio  0.20775897341573138\n",
      "Value to split on  1.8\n",
      "\n",
      "Level  2\n",
      "Count of 0 =  0\n",
      "Count of 1 =  49\n",
      "Count of 2 =  5\n",
      "Current entropy is  0.13397787198820432\n",
      "Splitting on feature  sepal length (cm)  with gain ratio  0.019926516393152488\n",
      "Value to split on  7.2\n",
      "\n",
      "Level  3\n",
      "Count of 0 =  0\n",
      "Count of 1 =  49\n",
      "Count of 2 =  4\n",
      "Current entropy is  0.11620326796476979\n",
      "Splitting on feature  sepal width (cm)  with gain ratio  0.018521175299525544\n",
      "Value to split on  2.9\n",
      "\n",
      "Level  4\n",
      "Count of  1  is =  27\n",
      "Majority vote is taken here\n",
      "Current Entropy is =  0.16700486810509502\n",
      "Reached leaf Node\n",
      "\n",
      "Level  4\n",
      "Count of  1  is =  22\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level  3\n",
      "Count of  2  is =  1\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level  2\n",
      "Count of  2  is =  45\n",
      "Majority vote is taken here\n",
      "Current Entropy is =  0.04548472037960742\n",
      "Reached leaf Node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calling function to print output\n",
    "\n",
    "dt(X_train,Y_train,features,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
